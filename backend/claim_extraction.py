import os, json, torch, re
from tqdm import tqdm
from datetime import datetime
from transformers import (
    pipeline,
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)

# ======================================
# üîß Config
# ======================================
os.environ["TRANSFORMERS_SAFE_MODEL_FORCING"] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

BASE_DIR = os.path.dirname(__file__)
DATA_DIR = os.path.join(BASE_DIR, "data")

INPUT_FILE = os.path.join(DATA_DIR, "crawled_sentences.json")
OUTPUT_FILE = os.path.join(DATA_DIR, "claims_output.json")
CHECKPOINT_FILE = os.path.join(DATA_DIR, "claims_checkpoint.json")
OFFLOAD_DIR = os.path.join(BASE_DIR, "offload_cache")

MODEL = "Qwen/Qwen3-4B-Instruct-2507"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(OFFLOAD_DIR, exist_ok=True)

# ======================================
# ‚öôÔ∏è Load Model
# ======================================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

print(f"[{datetime.now().strftime('%H:%M:%S')}] üîÑ Loading model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL,
    device_map="auto",
    quantization_config=bnb_config,
    low_cpu_mem_usage=True,
    offload_folder=OFFLOAD_DIR,
    trust_remote_code=True,
)
model.eval()

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    return_full_text=False,  
)

print(f"[{datetime.now().strftime('%H:%M:%S')}] ‚úÖ Model loaded!\n")

DEBUG = True

# ======================================
# üß† Single-Sentence JSON Classifier
# ======================================
@torch.inference_mode()
def classify_sentence(sentence: str) -> bool:
    prompt = f"""
B·∫°n l√† h·ªá th·ªëng ph√¢n lo·∫°i c√¢u.

Nhi·ªám v·ª•:
X√°c ƒë·ªãnh xem c√¢u sau c√≥ ph·∫£i l√† m·ªôt ph√°t bi·ªÉu mang t√≠nh th·ª±c t·∫ø,
m√¥ t·∫£ m·ªôt s·ª± ki·ªán, h√†nh ƒë·ªông, con s·ªë ho·∫∑c th√¥ng tin kh√°ch quan
m√† v·ªÅ nguy√™n t·∫Øc c√≥ th·ªÉ ki·ªÉm ch·ª©ng ƒë∆∞·ª£c.

KH√îNG ƒë∆∞·ª£c xem l√† ph√°t bi·ªÉu th·ª±c t·∫ø:
- √Ω ki·∫øn c√° nh√¢n
- c·∫£m x√∫c
- d·ª± ƒëo√°n
- ph·ªèng ƒëo√°n
- l·ªùi khuy√™n
- nh·∫≠n x√©t ch·ªß quan
- c√¢u ƒë√πa ho·∫∑c v√≠ d·ª• minh ho·∫°
- tr√≠ch d·∫´n b√¨nh lu·∫≠n c·ªßa ng∆∞·ªùi kh√°c

C√¢u: "{sentence}"

TR·∫¢ L·ªúI THEO ƒê√öNG ƒê·ªäNH D·∫†NG JSON CH·ªà G·ªíM:
{{ "answer": "YES" }}
ho·∫∑c
{{ "answer": "NO" }}

Kh√¥ng vi·∫øt g√¨ kh√°c.
"""
    response = pipe(
        [{"role": "user", "content": prompt}],
        max_new_tokens=50,
        temperature=0.0,
        do_sample=False
    )[0]["generated_text"]

    print("-----")
    print("C√¢u:", sentence)
    print("Ph√¢n lo·∫°i:", response)
    print("-----\n")

    match = re.search(r'\{\s*"answer"\s*:\s*"(YES|NO)"\s*\}', response)
    if not match:
        return False

    return match.group(1) == "YES"

# ======================================
# üß† Extract claims (NO BATCH)
# ======================================
def extract_claims_from_sentences(sentences: list[str]) -> list[str]:
    claims = []
    for s in sentences:
        if classify_sentence(s):
            claims.append(s.strip())
    return claims


# ======================================
# üìÇ File helpers
# ======================================
def load_articles():
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
        return data if isinstance(data, list) else [data]

def load_existing(path):
    if not os.path.exists(path):
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            d = json.load(f)
            return d if isinstance(d, list) else []
    except:
        return []

def save_checkpoint(results, path):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    os.replace(tmp, path)


# ======================================
# üöÄ Main
# ======================================
def main():
    articles = load_articles()
    print(f"üìÇ Loaded {len(articles)} sentence-level articles")

    results = load_existing(OUTPUT_FILE)
    checkpoint = load_existing(CHECKPOINT_FILE)

    merged = {r["id"]: r for r in (results + checkpoint) if "id" in r}
    done_ids = set(merged.keys())

    start = datetime.now()

    for article in tqdm(articles, desc="üß† Classifying sentences"):
        article_id = article["id"]
        if article_id in done_ids:
            continue

        sentences = article.get("sentences", [])
        if not sentences:
            continue

        claims = extract_claims_from_sentences(sentences)
        article_url = article.get("url", "")

        merged[article_id] = {
            "id": article_id,
            "url": article_url,
            "claims": claims,
            "timestamp": datetime.now().isoformat(timespec="seconds"),
        }

        if len(merged) % 5 == 0:
            save_checkpoint(list(merged.values()), CHECKPOINT_FILE)

    save_checkpoint(list(merged.values()), OUTPUT_FILE)

    print(f"\n‚úÖ Done! Saved {len(merged)} results ‚Üí {OUTPUT_FILE}")
    print("‚è±Ô∏è Runtime:", datetime.now() - start)


if __name__ == "__main__":
    main()
