import os, json, re, random
from datetime import datetime
from typing import List, Dict, Any, Optional

import torch
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

# -----------------------------
# CONFIG
# -----------------------------
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
OFFLOAD_DIR = os.path.join(os.path.dirname(__file__), "model_offload")
os.makedirs(OFFLOAD_DIR, exist_ok=True)

EMBED_MODEL = "intfloat/multilingual-e5-base"
EMBED_MIN_LEN = 80
SUMMARY_MIN_LEN = 50

FIXED_INTRO = "Nhá»¯ng thÃ´ng tin dÆ°á»›i Ä‘Ã¢y Ä‘Æ°á»£c tá»•ng há»£p tá»« cÃ¡c dá»¯ liá»‡u hiá»‡n cÃ³ vÃ  Ä‘Æ°á»£c trÃ¬nh bÃ y theo cÃ¡ch trung láº­p, khÃ¡ch quan nháº¥t cÃ³ thá»ƒ."
FIXED_DISCLAIMER = "DÃ n Ã½ dÆ°á»›i Ä‘Ã¢y nháº±m há»‡ thá»‘ng hÃ³a ná»™i dung tá»« cÃ¡c nguá»“n liÃªn quan vÃ  khÃ´ng thay tháº¿ cho Ä‘Ã¡nh giÃ¡ Ä‘áº§y Ä‘á»§ cá»§a chuyÃªn gia hay cÆ¡ quan chá»©c nÄƒng."
FIXED_CLOSING = "DÃ¹ ná»™i dung thuá»™c lÄ©nh vá»±c nÃ o, viá»‡c tiáº¿p cáº­n thÃ´ng tin má»™t cÃ¡ch tháº­n trá»ng vÃ  dá»±a trÃªn nguá»“n Ä‘Ã¡ng tin cáº­y luÃ´n lÃ  yáº¿u tá»‘ quan trá»ng."
FIXED_SIGNATURE = "ChÃºng tÃ´i sáº½ tiáº¿p tá»¥c cáº­p nháº­t khi cÃ³ thÃªm dá»¯ liá»‡u má»›i hoáº·c cÃ¡c thÃ´ng tin liÃªn quan."

HEALTH_KEYWORDS = [
    "ung thÆ°", "carotenoid", "dá»‹ch tá»…", "phÃ²ng ngá»«a", "lá»‘i sá»‘ng", "vaccine",
    "tim máº¡ch", "Ä‘Ã¡i thÃ¡o Ä‘Æ°á»ng", "huyáº¿t Ã¡p", "sá»©c khá»e", "khÃ¡ng sinh",
    "bá»‡nh", "triá»‡u chá»©ng", "cháº©n Ä‘oÃ¡n"
]

# -----------------------------
# MODEL LOAD (module-level)
# -----------------------------
print(f"[generate_article] [{datetime.now().strftime('%H:%M:%S')}] ğŸ”„ Loading Qwen model ({MODEL_NAME})...")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# load tokenizer & model
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    quantization_config=bnb_config,
    low_cpu_mem_usage=True,
    offload_folder=OFFLOAD_DIR,
    trust_remote_code=True,
)
model.eval()

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    return_full_text=False,
)

print(f"[generate_article] [{datetime.now().strftime('%H:%M:%S')}] âœ… Model ready.")

# -----------------------------
# EMBEDDING MODEL
# -----------------------------
_embed_model = SentenceTransformer(EMBED_MODEL)


def embed_claims(claims: List[str]):
    if not claims:
        return None
    return _embed_model.encode(claims, normalize_embeddings=True)


# -----------------------------
# CLUSTERING
# -----------------------------
def cluster_claims(embeddings, distance_threshold=0.8):
    clustering = AgglomerativeClustering(
        n_clusters=None,
        distance_threshold=distance_threshold,
        metric="cosine",
        linkage="average",
    )
    return clustering.fit_predict(embeddings)


def group_by_label(claims: List[str], labels: List[int]) -> Dict[int, List[str]]:
    groups: Dict[int, List[str]] = {}
    for c, l in zip(claims, labels):
        groups.setdefault(int(l), []).append(c)
    return groups


# -----------------------------
# LLM wrapper
# -----------------------------
def llm(prompt: str, max_tokens: int = 400) -> str:
    out = pipe(
        [{"role": "user", "content": prompt}],
        max_new_tokens=max_tokens,
        do_sample=False,
        temperature=0.1,
        repetition_penalty=1.05,
    )
    return out[0].get("generated_text", "").strip()


# -----------------------------
# Safe templates for expansion (no LLM)
# -----------------------------
UNIVERSAL_REASONING = [
    "Äiá»u nÃ y cho tháº¥y tÃ¬nh tiáº¿t nÃ y cÃ³ vai trÃ² Ä‘Ã¡ng chÃº Ã½ trong toÃ n bá»™ bá»‘i cáº£nh.",
    "Chi tiáº¿t nÃ y pháº£n Ã¡nh má»™t pháº§n diá»…n biáº¿n cáº§n Ä‘Æ°á»£c quan tÃ¢m thÃªm.",
    "Äiá»u nÃ y nháº¥n máº¡nh ráº±ng cÃ¡c sá»± viá»‡c cÃ³ thá»ƒ phá»©c táº¡p hÆ¡n so vá»›i quan sÃ¡t ban Ä‘áº§u.",
    "ThÃ´ng tin nÃ y giÃºp lÃ m rÃµ hÆ¡n diá»…n biáº¿n chung cá»§a sá»± viá»‡c.",
    "ÄÃ¢y lÃ  má»™t yáº¿u tá»‘ quan trá»ng trong viá»‡c hiá»ƒu toÃ n bá»™ cÃ¢u chuyá»‡n.",
]

UNIVERSAL_CONTEXT = [
    "má»™t chi tiáº¿t cáº§n Ä‘Æ°á»£c theo dÃµi ká»¹ hÆ¡n.",
    "má»™t pháº§n cá»§a bá»©c tranh tá»•ng thá»ƒ Ä‘ang dáº§n hÃ© lá»™.",
    "má»™t khÃ­a cáº¡nh thá»ƒ hiá»‡n chiá»u sÃ¢u cá»§a sá»± viá»‡c.",
    "má»™t dáº¥u hiá»‡u cho tháº¥y cÃ¢u chuyá»‡n cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n nhiá»u yáº¿u tá»‘ khÃ¡c nhau.",
    "má»™t Ä‘iá»ƒm Ä‘Ã¡ng chÃº Ã½ khi Ä‘áº·t cáº¡nh cÃ¡c claim khÃ¡c.",
]

UNIVERSAL_CONNECTORS = [
    "BÃªn cáº¡nh Ä‘Ã³,",
    "NgoÃ i ra,",
    "á» má»™t diá»…n biáº¿n liÃªn quan,",
    "Theo ghi nháº­n,",
    "Trong bá»‘i cáº£nh Ä‘Ã³,",
]

def expand_claim_safe(claim: str) -> str:
    c = (claim or "").strip()
    if not c:
        return c
    if len(c) >= 90:
        return c

    # safe expansion pieces
    part1 = random.choice(UNIVERSAL_REASONING)
    part2 = random.choice(UNIVERSAL_CONTEXT)
    connector = random.choice(UNIVERSAL_CONNECTORS)

    expanded = f"{c}. {part1} ÄÃ¢y lÃ  {part2}"
    if random.random() < 0.35:
        expanded = f"{connector} {expanded[0].lower() + expanded[1:]}"
    return expanded

# -----------------------------
# JSON helpers
# -----------------------------
_JSON_OBJ_RE = re.compile(r"\{[\s\S]*?\}")


def extract_first_json(raw: str) -> Optional[str]:
    if not raw:
        return None
    stack = 0
    start = None
    for i, ch in enumerate(raw):
        if ch == "{":
            if stack == 0:
                start = i
            stack += 1
        elif ch == "}":
            if stack > 0:
                stack -= 1
                if stack == 0 and start is not None:
                    return raw[start:i + 1]
    m = _JSON_OBJ_RE.search(raw)
    return m.group(0) if m else None


def safe_load_json(raw: str) -> Optional[dict]:
    if not raw:
        return None
    try:
        return json.loads(raw)
    except Exception:
        snippet = extract_first_json(raw)
        if not snippet:
            return None
        try:
            return json.loads(snippet)
        except Exception:
            return None


# -----------------------------
# Topic naming: LLM suggests N names (one per group). We map deterministically.
# -----------------------------
def name_topics_safely(num_groups: int) -> List[str]:
    prompt = f"""
Báº¡n lÃ  trá»£ lÃ½ biÃªn táº­p. HÃ£y Ä‘áº·t {num_groups} tÃªn chá»§ Ä‘á» NGáº®N (1â€“5 tá»«), má»—i tÃªn trÃªn 1 dÃ²ng.
YÃŠU Cáº¦U:
- KhÃ´ng giáº£i thÃ­ch.
- TrÃ¡nh cÃ¡c cá»¥m chung chung nhÆ° 'tÃ¡c dá»¥ng thuá»‘c', 'nguyÃªn nhÃ¢n bá»‡nh', 'lá»‘i sá»‘ng'.
- KhÃ´ng Ä‘Æ°á»£c thÃªm claim hay phÃ¢n tÃ­ch.
"""
    raw = llm(prompt, max_tokens=80)
    lines = [l.strip("-â€¢ ").strip() for l in raw.split("\n") if l.strip()]
    # ensure count
    while len(lines) < num_groups:
        lines.append(f"Chá»§ Ä‘á» {len(lines)+1}")
    return lines[:num_groups]


def refine_clusters_with_qwen(cluster_groups: Dict[int, List[str]]) -> Dict[str, List[str]]:
    # deterministic safe mapping: LLM provides N names, we map by order to preserve claims
    num = len(cluster_groups)
    names = name_topics_safely(num)
    refined: Dict[str, List[str]] = {}
    for (cid, clist), name in zip(cluster_groups.items(), names):
        refined[name] = clist
    return refined


# -----------------------------
# Strict rewrite (use LLM but forbid adding facts)
# -----------------------------
def rewrite_claim_strict(claim: str) -> str:
    prompt = f"""
Viáº¿t láº¡i cÃ¢u sau rÃµ rÃ ng hÆ¡n nhÆ°ng KHÃ”NG Ä‘Æ°á»£c thÃªm báº¥t ká»³ thÃ´ng tin má»›i nÃ o:
"{claim}"
YÃŠU Cáº¦U:
- 1 cÃ¢u duy nháº¥t.
- KhÃ´ng thÃªm nguyÃªn nhÃ¢n, khÃ´ng thÃªm cháº©n Ä‘oÃ¡n, khÃ´ng suy Ä‘oÃ¡n.
- Giá»¯ nguyÃªn cÃ¡c thá»±c táº¿ trong cÃ¢u gá»‘c.
Báº®T Äáº¦U:
"""
    out = llm(prompt, max_tokens=60).strip()
    out = out.strip("-â€¢ ").strip()
    return out if out else claim


def rewrite_all_claims_strict(claims: List[str]) -> List[str]:
    return [rewrite_claim_strict(c) for c in claims]


# -----------------------------
# Enrichment: expand short claims using safe templates (no LLM)
# -----------------------------
def enrich_claims_safe(claims: List[str]) -> List[str]:
    return [expand_claim_safe(c) for c in claims]


def enrich_summary(summary: str) -> str:
    if not summary:
        return ""
    if len(summary.strip()) < SUMMARY_MIN_LEN:
        p = f"""
Viáº¿t láº¡i tÃ³m táº¯t sau cho rÃµ rÃ ng, Ä‘áº§y Ä‘á»§ hÆ¡n nhÆ°ng KHÃ”NG thÃªm thÃ´ng tin má»›i:
"{summary}"
"""
        out = llm(p, max_tokens=80).strip()
        return out if out else summary
    return summary


# -----------------------------
# Balance single-claim topics (optional LLM but safe)
# -----------------------------
def balance_topics(topics: Dict[str, List[str]]) -> Dict[str, List[str]]:
    balanced: Dict[str, List[str]] = {}
    for topic, clist in topics.items():
        if len(clist) == 1:
            claim = clist[0]
            # ask LLM to paraphrase 1-2 clarifying sentences, but forbid new facts
            p = f"""
DÆ°á»›i Ä‘Ã¢y lÃ  má»™t claim duy nháº¥t:
"{claim}"
HÃ£y viáº¿t 1â€“2 cÃ¢u diá»…n Ä‘áº¡t láº¡i Ä‘á»ƒ lÃ m rÃµ Ã½ nhÆ°ng KHÃ”NG Ä‘Æ°á»£c thÃªm thÃ´ng tin má»›i.
Má»—i cÃ¢u lÃ  má»™t Ä‘Æ¡n vá»‹ ngáº¯n.
"""
            out = llm(p, max_tokens=120).strip()
            lines = [l.strip() for l in re.split(r'\n|\.|\!|\?', out) if l.strip()]
            extras = [l for l in lines if len(l) > 10][:2]
            balanced[topic] = [claim] + extras
        else:
            balanced[topic] = clist
    return balanced


# -----------------------------
# Mode detection
# -----------------------------
def detect_mode_from_claims(claims: List[str]) -> str:
    text = " ".join(claims).lower()
    score = 0
    for kw in HEALTH_KEYWORDS:
        if kw in text:
            score += 1
    advisory_phrases = ["cáº§n", "nÃªn", "Ä‘á»ƒ phÃ²ng ngá»«a", "lá»i khuyÃªn", "giÃºp giáº£m", "lÃ m tÄƒng"]
    for ph in advisory_phrases:
        if ph in text:
            score += 1
    return "explainer" if score >= 2 else "news"


# -----------------------------
# Cleaners and outline helpers
# -----------------------------
def normalize_bullets(text: str) -> str:
    lines = text.split("\n")
    out_lines = []
    for l in lines:
        s = l.strip()
        if not s:
            continue
        s = re.sub(r'^(NhÃ³m|Group)\s*\d+\s*:\s*', '', s, flags=re.IGNORECASE)
        if re.match(r"^\d+\.\s+", s):
            s = "- " + re.sub(r"^\d+\.\s+", "", s)
        if s.startswith("* "):
            s = "- " + s[2:]
        out_lines.append(s)
    return "\n".join(out_lines)


def collapse_duplicate_blocks(text: str) -> str:
    if not text:
        return text
    text = text.replace("\r\n", "\n")
    paragraphs = [p.strip() for p in re.split(r'\n\s*\n', text) if p.strip()]
    if not paragraphs:
        return text.strip()
    n = len(paragraphs)
    for L in range(1, n//2 + 1):
        if n % L == 0:
            chunk = paragraphs[:L]
            repeats = [paragraphs[i:i+L] for i in range(0, n, L)]
            if all(r == chunk for r in repeats):
                return "\n\n".join(chunk).strip()
    return "\n\n".join(paragraphs).strip()


def clean_outline(text: str) -> str:
    if not text:
        return text
    text = re.sub(r'^(NhÃ³m|Group)\s*\d+\s*:\s*', '', text, flags=re.IGNORECASE | re.MULTILINE)
    text = normalize_bullets(text)
    text = collapse_duplicate_blocks(text)
    lines = [ln.strip() for ln in text.split("\n")]
    cleaned = []
    prev = None
    for ln in lines:
        if ln == prev:
            continue
        cleaned.append(ln)
        prev = ln
    cleaned_text = []
    for ln in cleaned:
        ln = re.sub(r'^(Giá»›i thiá»‡u ngáº¯n|Giá»›i thiá»‡u):', 'Giá»›i thiá»‡u:', ln, flags=re.IGNORECASE)
        ln = re.sub(r'^(CÃ¡c luáº­n Ä‘iá»ƒm chÃ­nh|Ná»™i dung chÃ­nh|Ná»™i dung):', 'Ná»™i dung chÃ­nh:', ln, flags=re.IGNORECASE)
        ln = re.sub(r'^(Káº¿t luáº­n ná»™i dung|Káº¿t luáº­n):', 'Káº¿t luáº­n ná»™i dung:', ln, flags=re.IGNORECASE)
        cleaned_text.append(ln)
    return "\n".join(cleaned_text).strip()


# -----------------------------
# Outline generators (strict)
# -----------------------------
def generate_outline_sections_explainer(topics: Dict[str, List[str]]) -> str:
    prompt = f"""
Báº¡n lÃ  biÃªn táº­p viÃªn ná»™i dung dáº¡ng giáº£i thÃ­ch (explainer).
DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c topic vá»›i claim (Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a):

{json.dumps(topics, ensure_ascii=False, indent=2)}

YÃŠU Cáº¦U Báº®T BUá»˜C:
1) Má»—i gáº¡ch Ä‘áº§u dÃ²ng PHáº¢I dá»±a TRá»°C TIáº¾P tá»« claim (trÃ­ch hoáº·c diá»…n Ä‘áº¡t láº¡i).
2) KHÃ”NG Ä‘Æ°á»£c suy Ä‘oÃ¡n, KHÃ”NG thÃªm thÃ´ng tin má»›i.
3) NGHIÃŠM Cáº¤M viáº¿t cÃ¡c tiÃªu Ä‘á» chung nhÆ° "TÃ¡c dá»¥ng cá»§a thuá»‘c" hoáº·c "NguyÃªn nhÃ¢n bá»‡nh".
4) Giá»¯ cáº¥u trÃºc 3 pháº§n: Giá»›i thiá»‡u; Ná»™i dung chÃ­nh; Káº¿t luáº­n ná»™i dung.
5) Má»—i pháº§n 2â€“4 gáº¡ch Ä‘áº§u dÃ²ng, báº¯t Ä‘áº§u báº±ng "- ".
Báº®T Äáº¦U:
"""
    raw = llm(prompt, max_tokens=380)
    if not raw or not raw.strip():
        return (
            "Giá»›i thiá»‡u:\n- [KhÃ´ng Ä‘á»§ dá»¯ liá»‡u]\n\n"
            "Ná»™i dung chÃ­nh:\n- [KhÃ´ng Ä‘á»§ dá»¯ liá»‡u]\n\n"
            "Káº¿t luáº­n ná»™i dung:\n- [KhÃ´ng Ä‘á»§ dá»¯ liá»‡u]"
        )
    return clean_outline(raw)


def generate_outline_sections_news(topics: Dict[str, List[str]]) -> str:
    prompt = f"""
Báº¡n lÃ  biÃªn táº­p viÃªn tin tá»©c. Dá»±a trÃªn dá»¯ liá»‡u dÆ°á»›i Ä‘Ã¢y (topic -> claims), hÃ£y viáº¿t dÃ n Ã½ ngáº¯n gá»n, factual, khÃ´ng suy Ä‘oÃ¡n.

Dá»¯ liá»‡u:
{json.dumps(topics, ensure_ascii=False, indent=2)}

YÃŠU Cáº¦U:
- 3 pháº§n: Bá»‘i cáº£nh; Diá»…n biáº¿n chÃ­nh; PhÃ¢n tÃ­ch / TÃ¡c Ä‘á»™ng.
- Má»—i pháº§n 2â€“4 gáº¡ch Ä‘áº§u dÃ²ng, báº¯t Ä‘áº§u báº±ng "-".
- Chá»‰ dÃ¹ng thÃ´ng tin cÃ³ trong claim, khÃ´ng thÃªm dá»¯ liá»‡u.
Báº¯t Ä‘áº§u viáº¿t:
"""
    raw = llm(prompt, max_tokens=380)
    if not raw or not raw.strip():
        return "Bá»‘i cáº£nh:\n- [KhÃ´ng Ä‘á»§ dá»¯ liá»‡u]\n\nDiá»…n biáº¿n chÃ­nh:\n- [KhÃ´ng Ä‘á»§ dá»¯ liá»‡u]\n\nPhÃ¢n tÃ­ch / TÃ¡c Ä‘á»™ng:\n- [KhÃ´ng Ä‘á»§ dá»¯ liá»‡u]"
    return clean_outline(raw)


# -----------------------------
# Hook generator
# -----------------------------
def generate_hook(summary: str, entity: str = "nhÃ¢n váº­t") -> str:
    if not summary or not summary.strip():
        return f"CÃ¢u chuyá»‡n vá» {entity} Ä‘ang thu hÃºt sá»± chÃº Ã½."
    prompt = f"""
Báº¡n lÃ  biÃªn táº­p viÃªn tin tá»©c.
Viáº¿t 1 cÃ¢u hook má»Ÿ Ä‘áº§u dá»±a CHá»ˆ trÃªn tÃ³m táº¯t sau (khÃ´ng thÃªm thÃ´ng tin má»›i):

TÃ“M Táº®T: "{summary}"

YÃŠU Cáº¦U:
- 1 cÃ¢u duy nháº¥t.
- KhÃ´ng thÃªm, khÃ´ng suy Ä‘oÃ¡n.
- Tá»± nhiÃªn, rÃµ nghÄ©a.
Báº¯t Ä‘áº§u viáº¿t:
"""
    raw = llm(prompt, max_tokens=80)
    s = raw.replace("\n", " ").strip()
    s = s.strip().strip('"').strip("'").strip()
    if "." in s:
        s = s.split(".")[0].strip() + "."
    s = re.sub(r'\s+,', ',', s)
    s = re.sub(r'\s{2,}', ' ', s)
    return s if s else f"CÃ¢u chuyá»‡n vá» {entity} Ä‘ang thu hÃºt sá»± chÃº Ã½."


# -----------------------------
# Final assembly
# -----------------------------
def assemble_final_output(hook: str, middle: str) -> str:
    middle_clean = clean_outline(middle)
    if not middle_clean.startswith("- " + FIXED_DISCLAIMER):
        middle_block = f"- {FIXED_DISCLAIMER}\n{middle_clean}"
    else:
        middle_block = middle_clean
    final = f"""
ğŸ¬ DÃ€N Ã VIDEO (3â€“4 PHÃšT)

1. Má»Ÿ Ä‘áº§u (HOOK):
- {FIXED_INTRO}
- {hook}

2. CÃ¡c pháº§n ná»™i dung:
{middle_block}

3. Káº¿t luáº­n:
- {FIXED_CLOSING}
- {FIXED_SIGNATURE}
""".strip()
    lines = [ln.rstrip() for ln in final.split("\n")]
    cleaned_lines = []
    prev_blank = False
    for ln in lines:
        if ln.strip() == "":
            if not prev_blank:
                cleaned_lines.append("")
            prev_blank = True
        else:
            cleaned_lines.append(ln.strip())
            prev_blank = False
    if cleaned_lines and cleaned_lines[0] == "":
        cleaned_lines = cleaned_lines[1:]
    if cleaned_lines and cleaned_lines[-1] == "":
        cleaned_lines = cleaned_lines[:-1]
    final_text = "\n".join(cleaned_lines)

    return final_text


# -----------------------------
# Top-level: build_full_outline
# -----------------------------
def build_full_outline(data: Dict[str, Any]) -> str:
    summary = (data.get("summary") or "").strip()
    claims = data.get("claims") or []
    entities = data.get("entities") or []

    # 1) strict rewrite (no new facts)
    claims = rewrite_all_claims_strict(claims)

    # 2) enrichment: expand short claims using safe templates
    claims = [expand_claim_safe(c) for c in claims]
    mode = detect_mode_from_claims(claims)

    # 3) optional summary enrichment
    if summary:
        summary = enrich_summary(summary)

    # fallback: few claims -> summary-based outline
    if len(claims) < 2 and summary:
        hook = generate_hook(summary, entities[0] if entities else "nhÃ¢n váº­t")
        middle = generate_summary_based_outline(summary, entities)
        return assemble_final_output(hook, middle)

    if not claims and not summary:
        hook = f"CÃ¢u chuyá»‡n vá» {entities[0] if entities else 'nhÃ¢n váº­t'} Ä‘ang thu hÃºt sá»± chÃº Ã½."
        middle = "Bá»‘i cáº£nh:\n- [KhÃ´ng Ä‘á»§ dá»¯ liá»‡u]\n\nDiá»…n biáº¿n chÃ­nh:\n- [KhÃ´ng Ä‘á»§ dá»¯ liá»‡u]\n\nPhÃ¢n tÃ­ch / TÃ¡c Ä‘á»™ng:\n- [KhÃ´ng Ä‘á»§ dá»¯ liá»‡u]"
        return assemble_final_output(hook, middle)

    # embed + cluster
    embeddings = embed_claims(claims) if claims else None
    if embeddings is None or getattr(embeddings, 'size', None) == 0 or len(embeddings) == 0:
        rough_groups = {0: claims}
    else:
        labels = cluster_claims(embeddings)
        rough_groups = group_by_label(claims, labels)

    # safe topic naming & mapping
    refined = refine_clusters_with_qwen(rough_groups)

    # balance topics (expand single-claim topics)
    refined = balance_topics(refined)

    # detect mode
    mode = detect_mode_from_claims(claims)

    # generate middle
    if mode == "explainer":
        middle = generate_outline_sections_explainer(refined)
    else:
        middle = generate_outline_sections_news(refined)

    # hook
    hook_src = summary if summary else " ".join(claims[:2])
    hook = generate_hook(hook_src, entities[0] if entities else "nhÃ¢n váº­t")

    return assemble_final_output(hook, middle)
